{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Un0rRNjsBC9D"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "# Step 1: Function to capture user's question\n",
        "def get_user_input():\n",
        "    return input(\"Enter your question: \")\n",
        "\n",
        "# Step 2: Extract Database Schema\n",
        "def get_database_schema(db_path):\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "    tables = cursor.fetchall()\n",
        "    schema = {}\n",
        "    for table in tables:\n",
        "        table = table[0]\n",
        "        cursor.execute(f\"PRAGMA table_info({table})\")\n",
        "        schema[table] = cursor.fetchall()\n",
        "    conn.close()\n",
        "    return schema\n",
        "\n",
        "# Step 3: Create Keyword to Schema Mapping\n",
        "def generate_schema_mapping(db_path):\n",
        "    schema = get_database_schema(db_path)\n",
        "    schema_mapping = {}\n",
        "    for table, columns in schema.items():\n",
        "        for column in columns:\n",
        "            column_name = column[1].lower()\n",
        "            schema_mapping[column_name] = (table, column_name)\n",
        "    return schema_mapping\n",
        "\n",
        "# Step 4: Map user's question to relevant columns and tables\n",
        "def refined_map_keywords_to_schema(question, schema_mapping):\n",
        "    tokens = set(question.lower().split())\n",
        "    tables = set()\n",
        "    columns = set()\n",
        "    for token in tokens:\n",
        "        if token in schema_mapping:\n",
        "            table, column = schema_mapping[token]\n",
        "            tables.add(table)\n",
        "            columns.add(column)\n",
        "    return list(tables), list(columns)\n",
        "\n",
        "# Step 5: Generate a refined prompt for T5 model\n",
        "def refined_mapping_based_generate_prompt(question, schema_mapping):\n",
        "    relevant_tables, relevant_columns = refined_map_keywords_to_schema(question, schema_mapping)\n",
        "    if relevant_tables and relevant_columns:\n",
        "        tables_str = ', '.join(relevant_tables)\n",
        "        columns_str = ', '.join(relevant_columns)\n",
        "        prompt = f\"Translate the question '{question}' into SQL. Relevant tables: {tables_str}. Relevant columns: {columns_str}.\"\n",
        "    else:\n",
        "        prompt = question\n",
        "    return prompt\n",
        "\n",
        "# Step 6: Generate SQL using T5 model\n",
        "def get_sql(enhanced_prompt, tokenizer, model):\n",
        "    source_text = \"English to SQL: \" + enhanced_prompt\n",
        "    source_text = ' '.join(source_text.split())\n",
        "    source = tokenizer.batch_encode_plus([source_text], max_length=128, pad_to_max_length=True,\n",
        "                                         truncation=True, padding=\"max_length\", return_tensors='pt')\n",
        "    source_ids = source['input_ids']\n",
        "    source_mask = source['attention_mask']\n",
        "    generated_ids = model.generate(\n",
        "        input_ids=source_ids.to(dtype=torch.long),\n",
        "        attention_mask=source_mask.to(dtype=torch.long),\n",
        "        max_length=150,\n",
        "        num_beams=2,\n",
        "        repetition_penalty=2.5,\n",
        "        length_penalty=1.0,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "    return preds[0]\n",
        "\n",
        "# Step 7: Validate and correct the SQL\n",
        "def refined_validate_sql_against_schema(sql_query, expected_tables, expected_columns):\n",
        "    tokens = sql_query.split()\n",
        "    for i, token in enumerate(tokens):\n",
        "        if token.upper() == \"FROM\" and i + 1 < len(tokens) and tokens[i + 1] not in expected_tables:\n",
        "            if expected_tables:\n",
        "                tokens[i + 1] = expected_tables[0]\n",
        "        if token.upper() == \"SELECT\" and i + 1 < len(tokens) and tokens[i + 1] not in expected_columns:\n",
        "            if expected_columns:\n",
        "                tokens[i + 1] = expected_columns[0]\n",
        "    corrected_sql = ' '.join(tokens)\n",
        "    return corrected_sql\n",
        "\n",
        "# Main Workflow\n",
        "def enhanced_workflow_with_mapping(question, tokenizer, model, schema_mapping):\n",
        "    enhanced_prompt = refined_mapping_based_generate_prompt(question, schema_mapping)\n",
        "    sql_query = get_sql(enhanced_prompt, tokenizer, model)\n",
        "    expected_tables, expected_columns = refined_map_keywords_to_schema(question, schema_mapping)\n",
        "    corrected_sql = refined_validate_sql_against_schema(sql_query, expected_tables, expected_columns)\n",
        "    return corrected_sql\n",
        "\n",
        "# Example Usage:\n",
        "model = T5ForConditionalGeneration.from_pretrained('dsivakumar/text2sql')\n",
        "tokenizer = T5Tokenizer.from_pretrained('dsivakumar/text2sql')\n",
        "db_path = \"example-covid-vaccinations.sqlite3\"  # Replace with your actual database path\n",
        "schema_mapping = generate_schema_mapping(db_path)\n",
        "question = get_user_input()\n",
        "resulting_sql = enhanced_workflow_with_mapping(question, tokenizer, model, schema_mapping)\n",
        "print(resulting_sql)"
      ]
    }
  ]
}